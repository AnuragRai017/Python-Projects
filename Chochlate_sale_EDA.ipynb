{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOXTX47+FW+ac1ryBxqOfKO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuragRai017/Python-Projects/blob/master/Chochlate_sale_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Yobbb_zt8NL"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import scipy.stats as stats\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "import warnings\n",
        "\n",
        "# Configure settings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "try:\n",
        "    # Try reading with standard UTF-8 first\n",
        "    df = pd.read_csv('Chocolate Sales.csv')\n",
        "except UnicodeDecodeError:\n",
        "    # If that fails, try with utf-8-sig to handle potential BOM (Byte Order Mark)\n",
        "    df = pd.read_csv('Chocolate Sales.csv', encoding='utf-8-sig')\n",
        "\n",
        "print(\"--- Initial Data Head ---\")\n",
        "print(df.head())\n",
        "print(\"\\n--- Data Info ---\")\n",
        "df.info()\n",
        "print(\"\\n--- Initial Data Shape ---\")\n",
        "print(df.shape)\n",
        "\n",
        "# --- 2. Data Cleaning and Preprocessing ---\n",
        "\n",
        "# Clean column names (remove leading/trailing spaces, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n--- Missing Values ---\")\n",
        "print(df.isnull().sum())\n",
        "# No missing values detected initially, but cleaning might introduce them if formats are unexpected.\n",
        "\n",
        "# Clean 'Amount' column\n",
        "# Remove '$', ',', and spaces, then convert to numeric\n",
        "# Using errors='coerce' will turn unparseable values into NaN\n",
        "df['Amount'] = df['Amount'].astype(str).str.replace(r'[$,\\s]', '', regex=True)\n",
        "df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')\n",
        "\n",
        "# Convert 'Date' column to datetime objects\n",
        "# Use errors='coerce' for robustness\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y', errors='coerce')\n",
        "\n",
        "# Convert 'Boxes_Shipped' to numeric (just in case it was read as object)\n",
        "df['Boxes_Shipped'] = pd.to_numeric(df['Boxes_Shipped'], errors='coerce')\n",
        "\n",
        "# Check for NaNs introduced during cleaning\n",
        "print(\"\\n--- Missing Values After Cleaning ---\")\n",
        "print(df.isnull().sum())\n",
        "# If any NaNs were introduced, decide how to handle them (e.g., drop rows, impute)\n",
        "# For this example, let's drop rows with NaNs resulting from conversion errors\n",
        "df.dropna(subset=['Amount', 'Date', 'Boxes_Shipped'], inplace=True)\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\n--- Duplicate Rows ---\")\n",
        "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
        "# If duplicates exist, decide whether to remove them\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Feature Engineering: Extract time components\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Month_Name'] = df['Date'].dt.month_name()\n",
        "df['Day_Name'] = df['Date'].dt.day_name()\n",
        "df['Week_of_Year'] = df['Date'].dt.isocalendar().week.astype(int) # Use isocalendar().week\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "# Feature Engineering: Calculate Price per Box (handle potential division by zero)\n",
        "df['Price_Per_Box'] = df.apply(lambda row: row['Amount'] / row['Boxes_Shipped'] if row['Boxes_Shipped'] > 0 else 0, axis=1)\n",
        "# Remove potential outliers/infinite values if Boxes_Shipped was 0 (though we check > 0)\n",
        "df = df[np.isfinite(df['Price_Per_Box'])]\n",
        "\n",
        "\n",
        "print(\"\\n--- Data Info After Cleaning & Feature Engineering ---\")\n",
        "df.info()\n",
        "print(\"\\n--- Data Shape After Cleaning ---\")\n",
        "print(df.shape)\n",
        "print(\"\\n--- Descriptive Statistics ---\")\n",
        "# Include datetime=False for newer pandas versions if needed\n",
        "print(df.describe(include='number'))\n",
        "print(\"\\n--- Descriptive Statistics (Categorical) ---\")\n",
        "print(df.describe(include='object'))\n",
        "\n",
        "\n",
        "# --- 3. Exploratory Data Analysis (EDA) ---\n",
        "\n",
        "print(\"\\n--- Starting EDA Visualizations ---\")\n",
        "\n",
        "# 3.1. Distribution of Numerical Variables\n",
        "fig_dist = make_subplots(rows=1, cols=3, subplot_titles=('Amount Distribution', 'Boxes Shipped Distribution', 'Price Per Box Distribution'))\n",
        "\n",
        "fig_dist.add_trace(go.Histogram(x=df['Amount'], name='Amount', nbinsx=50), row=1, col=1)\n",
        "fig_dist.add_trace(go.Histogram(x=df['Boxes_Shipped'], name='Boxes Shipped', nbinsx=50), row=1, col=2)\n",
        "fig_dist.add_trace(go.Histogram(x=df['Price_Per_Box'], name='Price Per Box', nbinsx=50), row=1, col=3)\n",
        "\n",
        "fig_dist.update_layout(title_text=\"Distributions of Numerical Variables\", showlegend=False, height=400)\n",
        "fig_dist.show()\n",
        "\n",
        "# Observations:\n",
        "# - Amount seems right-skewed, most sales are lower amounts, with some high-value sales.\n",
        "# - Boxes Shipped also seems right-skewed.\n",
        "# - Price Per Box might reveal interesting patterns about product pricing or bulk discounts.\n",
        "\n",
        "# 3.2. Categorical Variable Analysis\n",
        "\n",
        "# Sales Person Performance (Top 10)\n",
        "top_sales_people = df.groupby('Sales_Person')['Amount'].sum().nlargest(10).reset_index()\n",
        "fig_sales_person = px.bar(top_sales_people, x='Sales_Person', y='Amount', title='Top 10 Sales People by Total Sales Amount',\n",
        "                          labels={'Amount':'Total Sales Amount ($)', 'Sales_Person':'Sales Person'})\n",
        "fig_sales_person.show()\n",
        "\n",
        "# Sales by Country\n",
        "country_sales = df.groupby('Country')['Amount'].sum().reset_index().sort_values('Amount', ascending=False)\n",
        "fig_country = px.bar(country_sales, x='Country', y='Amount', title='Total Sales Amount by Country',\n",
        "                     labels={'Amount':'Total Sales Amount ($)', 'Country':'Country'})\n",
        "fig_country.show()\n",
        "\n",
        "# Sales by Product (Top 10)\n",
        "top_products = df.groupby('Product')['Amount'].sum().nlargest(10).reset_index()\n",
        "fig_product = px.bar(top_products, x='Product', y='Amount', title='Top 10 Products by Total Sales Amount',\n",
        "                     labels={'Amount':'Total Sales Amount ($)', 'Product':'Product'})\n",
        "fig_product.update_xaxes(tickangle=45)\n",
        "fig_product.show()\n",
        "\n",
        "# Number of Sales Transactions by Country\n",
        "country_counts = df['Country'].value_counts().reset_index()\n",
        "country_counts.columns = ['Country', 'Count']\n",
        "fig_country_count = px.bar(country_counts, x='Country', y='Count', title='Number of Sales Transactions by Country',\n",
        "                           labels={'Count':'Number of Transactions', 'Country':'Country'})\n",
        "fig_country_count.show()\n",
        "\n",
        "# Average Amount per Box by Product (Top 10)\n",
        "avg_price_product = df.groupby('Product')['Price_Per_Box'].mean().nlargest(10).reset_index()\n",
        "fig_avg_price = px.bar(avg_price_product, x='Product', y='Price_Per_Box', title='Top 10 Products by Average Price Per Box',\n",
        "                      labels={'Price_Per_Box':'Average Price Per Box ($)', 'Product':'Product'})\n",
        "fig_avg_price.update_xaxes(tickangle=45)\n",
        "fig_avg_price.show()\n",
        "\n",
        "\n",
        "# 3.3. Relationship Analysis\n",
        "\n",
        "# Amount vs. Boxes Shipped\n",
        "fig_scatter = px.scatter(df, x='Boxes_Shipped', y='Amount', title='Amount vs. Boxes Shipped',\n",
        "                         labels={'Amount':'Sales Amount ($)', 'Boxes_Shipped':'Boxes Shipped'},\n",
        "                         hover_data=['Product', 'Country', 'Sales_Person'],\n",
        "                         trendline='ols', trendline_color_override='red') # Adding a trendline\n",
        "fig_scatter.show()\n",
        "\n",
        "# Calculate Correlation\n",
        "correlation = df['Amount'].corr(df['Boxes_Shipped'])\n",
        "print(f\"\\nCorrelation between Amount and Boxes Shipped: {correlation:.2f}\")\n",
        "# Observation: Expecting a positive correlation, but maybe not perfectly linear due to pricing variations.\n",
        "\n",
        "# Box plot of Amount by Country\n",
        "fig_box_country = px.box(df, x='Country', y='Amount', title='Sales Amount Distribution by Country',\n",
        "                        labels={'Amount':'Sales Amount ($)', 'Country':'Country'})\n",
        "fig_box_country.show()\n",
        "\n",
        "# Box plot of Amount by Product (Top 5 by median Amount)\n",
        "top_products_median = df.groupby('Product')['Amount'].median().nlargest(5).index\n",
        "df_top_prod_box = df[df['Product'].isin(top_products_median)]\n",
        "fig_box_product = px.box(df_top_prod_box, x='Product', y='Amount', title='Sales Amount Distribution for Top 5 Products (by Median Amount)',\n",
        "                         labels={'Amount':'Sales Amount ($)', 'Product':'Product'})\n",
        "fig_box_product.update_xaxes(tickangle=45)\n",
        "fig_box_product.show()\n",
        "\n",
        "\n",
        "# --- 4. Trend Analysis (Time Series) ---\n",
        "\n",
        "# Aggregate sales data by month\n",
        "df_monthly = df.set_index('Date').resample('M')[['Amount', 'Boxes_Shipped']].sum().reset_index()\n",
        "df_monthly['Month_Year'] = df_monthly['Date'].dt.strftime('%Y-%m') # For clearer labels\n",
        "\n",
        "fig_trend = make_subplots(rows=2, cols=1, shared_xaxes=True,\n",
        "                          subplot_titles=('Total Monthly Sales Amount', 'Total Monthly Boxes Shipped'))\n",
        "\n",
        "fig_trend.add_trace(go.Scatter(x=df_monthly['Date'], y=df_monthly['Amount'], mode='lines+markers', name='Amount'), row=1, col=1)\n",
        "fig_trend.add_trace(go.Scatter(x=df_monthly['Date'], y=df_monthly['Boxes_Shipped'], mode='lines+markers', name='Boxes Shipped'), row=2, col=1)\n",
        "\n",
        "fig_trend.update_layout(title_text='Monthly Sales Trends', height=600, hovermode='x unified')\n",
        "fig_trend.update_xaxes(title_text='Month', row=2, col=1)\n",
        "fig_trend.update_yaxes(title_text='Total Amount ($)', row=1, col=1)\n",
        "fig_trend.update_yaxes(title_text='Total Boxes Shipped', row=2, col=1)\n",
        "fig_trend.show()\n",
        "\n",
        "# Sales Trend by Day of Week\n",
        "day_sales = df.groupby('Day_Name')['Amount'].sum().reset_index()\n",
        "# Order days correctly\n",
        "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "day_sales['Day_Name'] = pd.Categorical(day_sales['Day_Name'], categories=day_order, ordered=True)\n",
        "day_sales = day_sales.sort_values('Day_Name')\n",
        "\n",
        "fig_day = px.bar(day_sales, x='Day_Name', y='Amount', title='Total Sales Amount by Day of the Week',\n",
        "                labels={'Amount':'Total Sales Amount ($)', 'Day_Name':'Day of Week'})\n",
        "fig_day.show()\n",
        "\n",
        "# Observation: Check if there are specific days with higher sales.\n",
        "\n",
        "# --- 5. Hypothesis Testing ---\n",
        "\n",
        "print(\"\\n--- Starting Hypothesis Testing ---\")\n",
        "alpha = 0.05 # Significance level\n",
        "\n",
        "# 5.1. Example 1: Is the average sales Amount significantly different between UK and USA? (Independent Samples T-test)\n",
        "\n",
        "# H0: The mean sales Amount is the same for UK and USA (μ_UK = μ_USA)\n",
        "# H1: The mean sales Amount is different for UK and USA (μ_UK ≠ μ_USA)\n",
        "\n",
        "# Extract data for the two groups\n",
        "amount_uk = df[df['Country'] == 'UK']['Amount']\n",
        "amount_usa = df[df['Country'] == 'USA']['Amount']\n",
        "\n",
        "# Check assumptions:\n",
        "# a) Normality (using Shapiro-Wilk test, suitable for smaller to moderate samples)\n",
        "shapiro_uk = stats.shapiro(amount_uk)\n",
        "shapiro_usa = stats.shapiro(amount_usa)\n",
        "print(f\"\\nShapiro-Wilk Test for Normality (UK Amount): W={shapiro_uk.statistic:.3f}, p={shapiro_uk.pvalue:.3f}\")\n",
        "print(f\"Shapiro-Wilk Test for Normality (USA Amount): W={shapiro_usa.statistic:.3f}, p={shapiro_usa.pvalue:.3f}\")\n",
        "\n",
        "# Visualization for Normality check\n",
        "fig_norm = make_subplots(rows=1, cols=2, subplot_titles=('UK Amount Distribution', 'USA Amount Distribution'))\n",
        "fig_norm.add_trace(go.Histogram(x=amount_uk, name='UK'), row=1, col=1)\n",
        "fig_norm.add_trace(go.Histogram(x=amount_usa, name='USA'), row=1, col=2)\n",
        "fig_norm.update_layout(title_text='Normality Check for T-test', showlegend=False)\n",
        "fig_norm.show()\n",
        "\n",
        "# If p-value < alpha, data is likely not normally distributed. T-test is robust to mild violations,\n",
        "# but if сильно non-normal, consider a non-parametric test (Mann-Whitney U).\n",
        "# Let's proceed with T-test for demonstration, noting the normality results.\n",
        "\n",
        "# b) Homogeneity of Variances (using Levene's test)\n",
        "levene_test = stats.levene(amount_uk, amount_usa)\n",
        "print(f\"\\nLevene's Test for Homogeneity of Variances: Statistic={levene_test.statistic:.3f}, p={levene_test.pvalue:.3f}\")\n",
        "\n",
        "# If Levene's test p < alpha, variances are unequal. Use Welch's T-test (equal_var=False).\n",
        "equal_variance = levene_test.pvalue >= alpha\n",
        "\n",
        "# Perform Independent Samples T-test\n",
        "t_stat, p_value_ttest = stats.ttest_ind(amount_uk, amount_usa, equal_var=equal_variance, nan_policy='omit')\n",
        "print(f\"\\nIndependent Samples T-test (UK vs USA Amount): T-statistic={t_stat:.3f}, p-value={p_value_ttest:.3f}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value_ttest < alpha:\n",
        "    print(f\"Conclusion: Reject H0. There is a statistically significant difference in the average sales Amount between UK and USA (p={p_value_ttest:.3f}).\")\n",
        "    # Compare means to see the direction\n",
        "    print(f\"Mean Amount UK: ${amount_uk.mean():.2f}, Mean Amount USA: ${amount_usa.mean():.2f}\")\n",
        "else:\n",
        "    print(f\"Conclusion: Fail to reject H0. There is no statistically significant difference in the average sales Amount between UK and USA (p={p_value_ttest:.3f}).\")\n",
        "\n",
        "# (Optional) Perform Mann-Whitney U test if normality assumption was strongly violated\n",
        "# mw_stat, p_value_mw = stats.mannwhitneyu(amount_uk, amount_usa, alternative='two-sided')\n",
        "# print(f\"\\nMann-Whitney U Test (UK vs USA Amount): Statistic={mw_stat:.3f}, p-value={p_value_mw:.3f}\")\n",
        "\n",
        "\n",
        "# 5.2. Example 2: Is the average sales Amount significantly different across Countries? (One-Way ANOVA)\n",
        "\n",
        "# H0: The mean sales Amount is the same across all countries (μ_UK = μ_USA = μ_Canada = ...)\n",
        "# H1: At least one country has a different mean sales Amount.\n",
        "\n",
        "# Check ANOVA assumptions:\n",
        "# a) Normality: We saw from the T-test check that data might not be perfectly normal. ANOVA is somewhat robust, but check overall.\n",
        "# b) Homogeneity of Variances: We can use Levene's test across all groups.\n",
        "\n",
        "# Prepare data for Levene's test and ANOVA\n",
        "country_groups = [df[df['Country'] == country]['Amount'] for country in df['Country'].unique()]\n",
        "country_names = df['Country'].unique()\n",
        "\n",
        "# Levene's test for all groups\n",
        "levene_anova = stats.levene(*country_groups)\n",
        "print(f\"\\nLevene's Test for Homogeneity of Variances (All Countries): Statistic={levene_anova.statistic:.3f}, p={levene_anova.pvalue:.3f}\")\n",
        "\n",
        "# If Levene's p < alpha, variances are unequal. Standard ANOVA might not be appropriate.\n",
        "# Consider Welch's ANOVA or Kruskal-Wallis test (non-parametric).\n",
        "# Let's proceed with standard ANOVA for demonstration, noting the Levene's result.\n",
        "\n",
        "# Perform One-Way ANOVA\n",
        "f_stat, p_value_anova = stats.f_oneway(*country_groups)\n",
        "print(f\"\\nOne-Way ANOVA (Amount across Countries): F-statistic={f_stat:.3f}, p-value={p_value_anova:.3e}\") # Using scientific notation for small p-values\n",
        "\n",
        "# Interpretation\n",
        "if p_value_anova < alpha:\n",
        "    print(f\"Conclusion: Reject H0. There is a statistically significant difference in the average sales Amount among at least two countries (p={p_value_anova:.3e}).\")\n",
        "    # If significant, follow up with post-hoc tests (e.g., Tukey HSD) to find which pairs differ.\n",
        "    # Post-hoc analysis is more involved and often done using statsmodels.\n",
        "    # Example (Conceptual - Requires statsmodels and careful setup):\n",
        "    # from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "    # tukey = pairwise_tukeyhsd(endog=df['Amount'], groups=df['Country'], alpha=alpha)\n",
        "    # print(\"\\nTukey HSD Post-Hoc Test:\")\n",
        "    # print(tukey)\n",
        "\n",
        "else:\n",
        "    print(f\"Conclusion: Fail to reject H0. There is no statistically significant difference in the average sales Amount across the different countries (p={p_value_anova:.3f}).\")\n",
        "\n",
        "\n",
        "# (Optional) Kruskal-Wallis H Test (Non-parametric alternative to ANOVA)\n",
        "# Useful if assumptions are violated.\n",
        "kw_stat, p_value_kw = stats.kruskal(*country_groups)\n",
        "print(f\"\\nKruskal-Wallis H Test (Amount across Countries): Statistic={kw_stat:.3f}, p-value={p_value_kw:.3e}\")\n",
        "if p_value_kw < alpha:\n",
        "    print(\"Kruskal-Wallis Conclusion: Reject H0. There is a statistically significant difference in the median sales Amount among at least two countries.\")\n",
        "else:\n",
        "     print(\"Kruskal-Wallis Conclusion: Fail to reject H0. No significant difference in median sales Amount across countries.\")\n",
        "\n",
        "\n",
        "# 5.3. Example 3: Is there an interaction effect between Country and Product on Sales Amount? (Two-Way ANOVA - Conceptual)\n",
        "# This tests if the effect of the Product type on sales Amount depends on the Country.\n",
        "# H0 (Interaction): The effect of Product on Amount is the same across all Countries (no interaction).\n",
        "# H1 (Interaction): The effect of Product on Amount differs depending on the Country.\n",
        "\n",
        "# Two-Way ANOVA requires more careful setup, often using statsmodels formula API.\n",
        "# It also benefits from having balanced groups or using appropriate ANOVA types (Type II or III).\n",
        "# This is a more advanced analysis. Let's set up a conceptual model:\n",
        "\n",
        "print(\"\\n--- Two-Way ANOVA (Conceptual Example) ---\")\n",
        "# We need enough data per Product*Country combination. Let's filter for more common products/countries if necessary.\n",
        "# For demonstration, we'll use the formula API with the existing data, but interpretation needs caution.\n",
        "\n",
        "try:\n",
        "    # The formula 'Amount ~ C(Country) + C(Product) + C(Country):C(Product)' tests main effects and interaction\n",
        "    # C() treats the variables as categorical\n",
        "    model = ols('Amount ~ C(Country) + C(Product) + C(Country):C(Product)', data=df).fit()\n",
        "    anova_table = anova_lm(model, typ=2) # Type 2 ANOVA is often suitable\n",
        "    print(\"\\nTwo-Way ANOVA Results (Amount ~ Country * Product):\")\n",
        "    print(anova_table)\n",
        "\n",
        "    # Interpretation: Look at the p-value (PR(>F)) for the interaction term 'C(Country):C(Product)'.\n",
        "    interaction_p_value = anova_table.loc['C(Country):C(Product)', 'PR(>F)']\n",
        "    if interaction_p_value < alpha:\n",
        "        print(f\"\\nConclusion (Interaction): Reject H0 for interaction. The effect of Product on sales Amount significantly depends on the Country (p={interaction_p_value:.3e}).\")\n",
        "        # Further analysis would involve examining interaction plots.\n",
        "    else:\n",
        "        print(f\"\\nConclusion (Interaction): Fail to reject H0 for interaction. No significant interaction effect found between Country and Product on sales Amount (p={interaction_p_value:.3f}).\")\n",
        "        # If no interaction, you can interpret the main effects (Country, Product) if they are significant.\n",
        "\n",
        "    # Visualize Interaction (Example using means - simplified)\n",
        "    interaction_data = df.groupby(['Country', 'Product'])['Amount'].mean().reset_index()\n",
        "    # Filter for top N products for clarity\n",
        "    top_product_names = df['Product'].value_counts().nlargest(5).index\n",
        "    interaction_data_filtered = interaction_data[interaction_data['Product'].isin(top_product_names)]\n",
        "\n",
        "    fig_interaction = px.line(interaction_data_filtered, x='Country', y='Amount', color='Product',\n",
        "                              title='Interaction Plot: Average Sales Amount by Country and Product (Top 5 Products)',\n",
        "                              labels={'Amount': 'Average Sales Amount ($)'}, markers=True)\n",
        "    fig_interaction.show()\n",
        "    # Look for non-parallel lines, which suggest an interaction.\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nCould not perform Two-Way ANOVA, possibly due to data structure or insufficient data per group. Error: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Analysis Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries (ensure these are already imported from the previous run)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import scipy.stats as stats\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose # For seasonality\n",
        "import warnings\n",
        "\n",
        "# Configure settings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# --- Assuming df and df_monthly are already loaded and preprocessed ---\n",
        "# If not, reload and preprocess as in the previous script.\n",
        "# For demonstration, let's assume 'df' and 'df_monthly' exist from the prior execution.\n",
        "# Make sure df has 'Amount', 'Boxes_Shipped', 'Price_Per_Box' as numeric\n",
        "# and 'Date' as datetime.\n",
        "# Make sure df_monthly has 'Date' index (or column) and 'Amount', 'Boxes_Shipped' columns.\n",
        "\n",
        "print(\"--- Dataframe info before further analysis ---\")\n",
        "# If df_monthly doesn't have Date as index, set it\n",
        "if not isinstance(df_monthly.index, pd.DatetimeIndex):\n",
        "     df_monthly = df_monthly.set_index('Date')\n",
        "\n",
        "df.info()\n",
        "print(\"\\nMonthly DataFrame Head:\")\n",
        "print(df_monthly.head())\n",
        "\n",
        "\n",
        "# --- 6. Handle Outliers ---\n",
        "\n",
        "print(\"\\n--- Outlier Analysis and Handling ---\")\n",
        "\n",
        "# 6.1. Identify Outliers using IQR method\n",
        "# Focus on Amount, Boxes_Shipped, Price_Per_Box\n",
        "numerical_cols = ['Amount', 'Boxes_Shipped', 'Price_Per_Box']\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    sns.boxplot(y=df[col])\n",
        "    plt.title(f'Box Plot: {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate IQR bounds\n",
        "outlier_info = {}\n",
        "for col in numerical_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    outlier_info[col] = {\n",
        "        'count': len(outliers),\n",
        "        'percentage': (len(outliers) / len(df)) * 100,\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound\n",
        "    }\n",
        "    print(f\"Outlier Info for '{col}':\")\n",
        "    print(f\"  - Count: {outlier_info[col]['count']}\")\n",
        "    print(f\"  - Percentage: {outlier_info[col]['percentage']:.2f}%\")\n",
        "    print(f\"  - IQR Lower Bound: {lower_bound:.2f}\")\n",
        "    print(f\"  - IQR Upper Bound: {upper_bound:.2f}\")\n",
        "\n",
        "# 6.2. Handle Outliers (Option 1: Log Transformation for Skewed Data like Amount/Boxes)\n",
        "# Log transformation reduces the impact of high-value outliers and skewness.\n",
        "# Use log1p to handle potential zeros gracefully.\n",
        "\n",
        "df['Amount_Log'] = np.log1p(df['Amount'])\n",
        "df['Boxes_Shipped_Log'] = np.log1p(df['Boxes_Shipped'])\n",
        "# Price_Per_Box might not be as skewed or might have a different interpretation.\n",
        "# Let's visualize the transformed distributions.\n",
        "\n",
        "fig_log_dist = make_subplots(rows=1, cols=2, subplot_titles=('Log(Amount+1) Distribution', 'Log(Boxes Shipped+1) Distribution'))\n",
        "fig_log_dist.add_trace(go.Histogram(x=df['Amount_Log'], name='Log Amount', nbinsx=50), row=1, col=1)\n",
        "fig_log_dist.add_trace(go.Histogram(x=df['Boxes_Shipped_Log'], name='Log Boxes Shipped', nbinsx=50), row=1, col=2)\n",
        "fig_log_dist.update_layout(title_text=\"Distributions after Log Transformation\", showlegend=False, height=400)\n",
        "fig_log_dist.show()\n",
        "\n",
        "print(\"\\nLog transformation applied to 'Amount' and 'Boxes_Shipped'.\")\n",
        "print(\"Note: For modeling, using these transformed variables might be beneficial.\")\n",
        "print(\"For general EDA reporting, often the original scale is kept, but outliers noted.\")\n",
        "\n",
        "# 6.3. Handle Outliers (Option 2: Capping - Optional, apply if transformation isn't sufficient or desired)\n",
        "# Example: Cap 'Price_Per_Box' outliers if they seem erroneous\n",
        "# upper_cap_price = outlier_info['Price_Per_Box']['upper_bound']\n",
        "# lower_cap_price = outlier_info['Price_Per_Box']['lower_bound']\n",
        "# df['Price_Per_Box_Capped'] = df['Price_Per_Box'].clip(lower=lower_cap_price, upper=upper_cap_price)\n",
        "# print(f\"\\nOutliers in 'Price_Per_Box' potentially capped between {lower_cap_price:.2f} and {upper_cap_price:.2f}\")\n",
        "# Note: For this analysis, we'll primarily use log transformation for Amount/Boxes and keep Price_Per_Box as is, acknowledging its outliers.\n",
        "\n",
        "\n",
        "# --- 7. Deeper Dive Analysis ---\n",
        "\n",
        "print(\"\\n--- Deeper Dive Analysis ---\")\n",
        "\n",
        "# 7.1. Analyze Top Selling Country (Example: USA - check your EDA results for the actual top country)\n",
        "# Find the top country by Amount if not already known\n",
        "top_country = df.groupby('Country')['Amount'].sum().idxmax()\n",
        "print(f\"\\nAnalyzing Top Selling Country: {top_country}\")\n",
        "\n",
        "df_top_country = df[df['Country'] == top_country].copy()\n",
        "\n",
        "# Trend within the top country\n",
        "df_top_country_monthly = df_top_country.set_index('Date').resample('M')['Amount'].sum().reset_index()\n",
        "fig_top_country_trend = px.line(df_top_country_monthly, x='Date', y='Amount',\n",
        "                                title=f'Monthly Sales Amount Trend in {top_country}',\n",
        "                                labels={'Amount': 'Total Sales Amount ($)'}, markers=True)\n",
        "fig_top_country_trend.show()\n",
        "\n",
        "# Top Products within the top country\n",
        "top_products_in_country = df_top_country.groupby('Product')['Amount'].sum().nlargest(10).reset_index()\n",
        "fig_top_products_country = px.bar(top_products_in_country, x='Product', y='Amount',\n",
        "                                  title=f'Top 10 Products by Sales Amount in {top_country}',\n",
        "                                  labels={'Amount': 'Total Sales Amount ($)'})\n",
        "fig_top_products_country.update_xaxes(tickangle=45)\n",
        "fig_top_products_country.show()\n",
        "\n",
        "# Top Sales Persons within the top country\n",
        "top_sales_persons_country = df_top_country.groupby('Sales_Person')['Amount'].sum().nlargest(10).reset_index()\n",
        "fig_top_sales_persons_country = px.bar(top_sales_persons_country, x='Sales_Person', y='Amount',\n",
        "                                       title=f'Top 10 Sales Persons by Sales Amount in {top_country}',\n",
        "                                       labels={'Amount': 'Total Sales Amount ($)'})\n",
        "fig_top_sales_persons_country.show()\n",
        "\n",
        "\n",
        "# 7.2. Analyze Top Selling Product (Example: Check your EDA results for the actual top product)\n",
        "top_product_name = df.groupby('Product')['Amount'].sum().idxmax()\n",
        "print(f\"\\nAnalyzing Top Selling Product: {top_product_name}\")\n",
        "\n",
        "df_top_product = df[df['Product'] == top_product_name].copy()\n",
        "\n",
        "# Trend for the top product\n",
        "df_top_product_monthly = df_top_product.set_index('Date').resample('M')['Amount'].sum().reset_index()\n",
        "fig_top_product_trend = px.line(df_top_product_monthly, x='Date', y='Amount',\n",
        "                                title=f'Monthly Sales Amount Trend for {top_product_name}',\n",
        "                                labels={'Amount': 'Total Sales Amount ($)'}, markers=True)\n",
        "fig_top_product_trend.show()\n",
        "\n",
        "# Sales of top product by Country\n",
        "top_product_by_country = df_top_product.groupby('Country')['Amount'].sum().reset_index().sort_values('Amount', ascending=False)\n",
        "fig_top_product_country = px.bar(top_product_by_country, x='Country', y='Amount',\n",
        "                                 title=f'Sales Amount of {top_product_name} by Country',\n",
        "                                 labels={'Amount': 'Total Sales Amount ($)'})\n",
        "fig_top_product_country.show()\n",
        "\n",
        "\n",
        "# --- 8. Segmentation Analysis (Conceptual) ---\n",
        "\n",
        "print(\"\\n--- Segmentation Analysis (Conceptual) ---\")\n",
        "# The column 'Customer Segment' (e.g., Retail, Wholesale) was mentioned but not present in the provided CSV structure.\n",
        "# If this column ('Customer_Segment') existed, we could perform analyses like:\n",
        "\n",
        "# 1. Compare Sales Metrics:\n",
        "#    - Box plot: px.box(df, x='Customer_Segment', y='Amount', title='Sales Amount by Customer Segment')\n",
        "#    - T-test/ANOVA: Test if average 'Amount' or 'Boxes_Shipped' differs significantly between segments.\n",
        "#      Example: stats.ttest_ind(df[df['Customer_Segment'] == 'Retail']['Amount'], df[df['Customer_Segment'] == 'Wholesale']['Amount'])\n",
        "\n",
        "# 2. Analyze Product Preferences by Segment:\n",
        "#    - Grouped bar chart showing top products for each segment.\n",
        "#      segment_product_sales = df.groupby(['Customer_Segment', 'Product'])['Amount'].sum().reset_index()\n",
        "#      # Filter for top products overall or per segment for clarity\n",
        "#      px.bar(segment_product_sales, x='Product', y='Amount', color='Customer_Segment', barmode='group', title='Product Sales by Segment')\n",
        "\n",
        "# 3. Analyze Purchase Frequency/Size:\n",
        "#    - Compare average 'Boxes_Shipped' per transaction for each segment.\n",
        "#    - Compare the number of transactions per segment over time.\n",
        "\n",
        "# 4. Time Trends by Segment:\n",
        "#    - Resample data monthly, grouped by segment, and plot trends.\n",
        "#      df_segment_monthly = df.groupby('Customer_Segment').resample('M', on='Date')['Amount'].sum().reset_index()\n",
        "#      px.line(df_segment_monthly, x='Date', y='Amount', color='Customer_Segment', title='Monthly Sales Trend by Segment')\n",
        "\n",
        "print(\"If 'Customer_Segment' data were available, the above analyses would provide insights into different customer groups.\")\n",
        "\n",
        "\n",
        "# --- 9. Seasonality Decomposition ---\n",
        "\n",
        "print(\"\\n--- Seasonality Decomposition ---\")\n",
        "\n",
        "# Ensure df_monthly index is DatetimeIndex and frequency is set (should be 'M' from resample)\n",
        "if df_monthly.index.freq is None:\n",
        "    # Infer frequency if possible, 'MS' (Month Start) or 'M' (Month End)\n",
        "    df_monthly.index.freq = pd.infer_freq(df_monthly.index)\n",
        "    print(f\"Inferred frequency for df_monthly: {df_monthly.index.freq}\")\n",
        "\n",
        "# Check if we have enough data points (at least 2 full cycles, e.g., 24 months for monthly data)\n",
        "required_periods = 2 * 12 # Adjust 12 based on expected cycle length (annual)\n",
        "if len(df_monthly) >= required_periods:\n",
        "    # Use multiplicative model as sales seasonality often scales with the trend\n",
        "    decomposition = seasonal_decompose(df_monthly['Amount'], model='multiplicative', period=12) # Assuming annual seasonality\n",
        "\n",
        "    # Plot the decomposition\n",
        "    decomp_fig = decomposition.plot()\n",
        "    plt.suptitle('Seasonal Decomposition of Monthly Sales Amount (Multiplicative)', y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "    # Create an interactive plot\n",
        "    decomp_data = pd.DataFrame({\n",
        "        'Date': df_monthly.index,\n",
        "        'Observed': decomposition.observed,\n",
        "        'Trend': decomposition.trend,\n",
        "        'Seasonal': decomposition.seasonal,\n",
        "        'Residual': decomposition.resid\n",
        "    }).dropna() # Drop NaNs from trend calculation at edges\n",
        "\n",
        "    fig_decomp_plotly = make_subplots(rows=4, cols=1, shared_xaxes=True,\n",
        "                                      subplot_titles=('Observed', 'Trend', 'Seasonal', 'Residual'))\n",
        "\n",
        "    fig_decomp_plotly.add_trace(go.Scatter(x=decomp_data['Date'], y=decomp_data['Observed'], mode='lines', name='Observed'), row=1, col=1)\n",
        "    fig_decomp_plotly.add_trace(go.Scatter(x=decomp_data['Date'], y=decomp_data['Trend'], mode='lines', name='Trend'), row=2, col=1)\n",
        "    fig_decomp_plotly.add_trace(go.Scatter(x=decomp_data['Date'], y=decomp_data['Seasonal'], mode='lines', name='Seasonal'), row=3, col=1)\n",
        "    fig_decomp_plotly.add_trace(go.Scatter(x=decomp_data['Date'], y=decomp_data['Residual'], mode='markers', name='Residual'), row=4, col=1)\n",
        "\n",
        "    fig_decomp_plotly.update_layout(title_text='Interactive Seasonal Decomposition', height=800, showlegend=False)\n",
        "    fig_decomp_plotly.show()\n",
        "\n",
        "else:\n",
        "    print(f\"Insufficient data for seasonal decomposition (requires at least {required_periods} periods, found {len(df_monthly)}). Skipping decomposition.\")\n",
        "\n",
        "\n",
        "# --- 10. Predictive Modeling (Discussion) ---\n",
        "\n",
        "print(\"\\n--- Predictive Modeling Discussion ---\")\n",
        "\n",
        "print(\"Based on the EDA and data structure, several predictive modeling tasks could be explored:\")\n",
        "\n",
        "print(\"\\n1. Sales Forecasting (Time Series):\")\n",
        "print(\"   - Goal: Predict future total sales amount or boxes shipped (e.g., next month, next quarter).\")\n",
        "print(\"   - Data: Use aggregated time series data (like df_monthly).\")\n",
        "print(\"   - Potential Models:\")\n",
        "print(\"     - Classical: ARIMA, SARIMA (if seasonality is strong), Exponential Smoothing (Holt-Winters).\")\n",
        "print(\"     - Machine Learning: Facebook Prophet (good with seasonality and holidays), LSTMs (for complex non-linear patterns, requires more data).\")\n",
        "print(\"   - Feature Engineering: Lag features (sales from previous periods), rolling statistics (moving averages), time features (month, quarter, year).\")\n",
        "print(\"   - Evaluation: MAE (Mean Absolute Error), RMSE (Root Mean Squared Error), MAPE (Mean Absolute Percentage Error).\")\n",
        "\n",
        "print(\"\\n2. Transaction Amount Prediction (Regression):\")\n",
        "print(\"   - Goal: Predict the 'Amount' for a single new sales transaction.\")\n",
        "print(\"   - Data: Use the original transaction-level data (df).\")\n",
        "print(\"   - Potential Models:\")\n",
        "print(\"     - Linear Models: Linear Regression (with regularization like Ridge or Lasso if many features).\")\n",
        "print(\"     - Tree-based Models: Random Forest, Gradient Boosting (XGBoost, LightGBM) - often perform well on tabular data.\")\n",
        "print(\"   - Feature Engineering: Convert categorical features (Product, Country, Sales_Person) into numerical representations (e.g., One-Hot Encoding, Target Encoding), use engineered time features (Month, Day_Name, Quarter). Use log-transformed Amount ('Amount_Log') as the target if distribution is skewed.\")\n",
        "print(\"   - Evaluation: R-squared, RMSE, MAE.\")\n",
        "\n",
        "print(\"\\nImportant Considerations for Modeling:\")\n",
        "print(\"   - Data Splitting: Properly split data into training, validation, and test sets (time-based split for forecasting).\")\n",
        "print(\"   - Feature Selection/Engineering: Crucial for model performance.\")\n",
        "print(\"   - Hyperparameter Tuning: Optimize model parameters using techniques like Grid Search or Randomized Search.\")\n",
        "print(\"   - Assumption Checks: Verify assumptions for models like Linear Regression.\")\n",
        "\n",
        "print(\"\\nBuilding these models requires further steps beyond this initial analysis.\")\n",
        "\n",
        "print(\"\\n--- Extended Analysis Complete ---\")"
      ],
      "metadata": {
        "id": "2bJI0qdswYbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports for Modeling ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "\n",
        "# Time Series Specific\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "# from pmdarima import auto_arima # Optional, for automatic order selection\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Regression Specific\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Configure settings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 7)\n",
        "\n",
        "# --- Load Data (Placeholder - Assume df and df_monthly exist) ---\n",
        "# If not loaded, uncomment and run the previous loading/preprocessing script first\n",
        "# print(\"Loading and preprocessing data...\")\n",
        "# ... (previous script code here) ...\n",
        "# print(\"Data loaded and preprocessed.\")\n",
        "\n",
        "# Ensure df_monthly has DatetimeIndex with frequency\n",
        "if not isinstance(df_monthly.index, pd.DatetimeIndex):\n",
        "    df_monthly = df_monthly.set_index('Date')\n",
        "if df_monthly.index.freq is None:\n",
        "    df_monthly.index.freq = pd.infer_freq(df_monthly.index)\n",
        "    if df_monthly.index.freq is None:\n",
        "         # Attempt setting monthly frequency if inference fails (common for end-of-month data)\n",
        "         df_monthly = df_monthly.asfreq('M')\n",
        "         print(\"Set df_monthly frequency to 'M'.\")\n",
        "    else:\n",
        "         print(f\"Inferred frequency for df_monthly: {df_monthly.index.freq}\")\n",
        "\n",
        "# --- 1. Sales Forecasting (Time Series) ---\n",
        "print(\"\\n--- Task 1: Sales Forecasting (Monthly Amount) ---\")\n",
        "\n",
        "# 1.1 Prepare Data\n",
        "ts_data = df_monthly[['Amount']].copy()\n",
        "ts_data = ts_data.dropna() # Ensure no NaNs\n",
        "\n",
        "# Check if enough data exists\n",
        "if len(ts_data) < 24: # Need sufficient data for train/test and seasonality\n",
        "    print(\"Warning: Insufficient monthly data points for robust forecasting.\")\n",
        "    # Optional: Proceed with caution or stop\n",
        "\n",
        "# 1.2 Train/Test Split (Time-based)\n",
        "test_periods = 3 # Predict last 3 months\n",
        "train_ts = ts_data[:-test_periods]\n",
        "test_ts = ts_data[-test_periods:]\n",
        "\n",
        "print(f\"Training data shape: {train_ts.shape}\")\n",
        "print(f\"Testing data shape: {test_ts.shape}\")\n",
        "\n",
        "# 1.3 Model 1: SARIMA Example\n",
        "print(\"\\n--- Fitting SARIMA Model ---\")\n",
        "# Note: Order selection (p,d,q)(P,D,Q,m) is crucial and often requires analysis\n",
        "# of ACF/PACF plots or using auto_arima. These are illustrative orders.\n",
        "# Common for monthly data with annual seasonality (m=12)\n",
        "# Let's try (1,1,1) for non-seasonal and (1,1,0,12) for seasonal parts.\n",
        "try:\n",
        "    sarima_model = SARIMAX(train_ts['Amount'],\n",
        "                           order=(1, 1, 1), # Non-seasonal order (p, d, q)\n",
        "                           seasonal_order=(1, 1, 0, 12), # Seasonal order (P, D, Q, m)\n",
        "                           enforce_stationarity=False,\n",
        "                           enforce_invertibility=False)\n",
        "    sarima_fit = sarima_model.fit(disp=False)\n",
        "    print(sarima_fit.summary())\n",
        "\n",
        "    # Forecast\n",
        "    sarima_forecast_obj = sarima_fit.get_forecast(steps=test_periods)\n",
        "    sarima_forecast = sarima_forecast_obj.predicted_mean\n",
        "    sarima_conf_int = sarima_forecast_obj.conf_int(alpha=0.05) # 95% confidence interval\n",
        "\n",
        "    # Evaluate\n",
        "    sarima_mae = mean_absolute_error(test_ts['Amount'], sarima_forecast)\n",
        "    sarima_rmse = np.sqrt(mean_squared_error(test_ts['Amount'], sarima_forecast))\n",
        "    print(f\"\\nSARIMA Forecast Evaluation:\")\n",
        "    print(f\"  MAE: {sarima_mae:.2f}\")\n",
        "    print(f\"  RMSE: {sarima_rmse:.2f}\")\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(ts_data.index, ts_data['Amount'], label='Observed')\n",
        "    plt.plot(test_ts.index, sarima_forecast, label='SARIMA Forecast', color='red')\n",
        "    plt.fill_between(test_ts.index,\n",
        "                     sarima_conf_int.iloc[:, 0],\n",
        "                     sarima_conf_int.iloc[:, 1], color='red', alpha=0.2, label='95% Confidence Interval')\n",
        "    plt.title('SARIMA Forecast vs Observed')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Monthly Sales Amount')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting/forecasting with SARIMA: {e}\")\n",
        "    print(\"SARIMA requires careful order selection and sufficient data.\")\n",
        "\n",
        "# 1.4 Model 2: Facebook Prophet Example\n",
        "print(\"\\n--- Fitting Prophet Model ---\")\n",
        "# Prepare data for Prophet\n",
        "prophet_train_df = train_ts.reset_index().rename(columns={'Date': 'ds', 'Amount': 'y'})\n",
        "\n",
        "# Instantiate and fit\n",
        "prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
        "# Add country-specific holidays if applicable (e.g., model.add_country_holidays(country_name='US'))\n",
        "prophet_model.fit(prophet_train_df)\n",
        "\n",
        "# Create future dataframe for prediction\n",
        "future_dates = prophet_model.make_future_dataframe(periods=test_periods, freq='M') # Use 'MS' if start of month\n",
        "prophet_forecast_df = prophet_model.predict(future_dates)\n",
        "\n",
        "# Isolate the forecast for the test period\n",
        "prophet_forecast_test = prophet_forecast_df.set_index('ds').iloc[-test_periods:]['yhat']\n",
        "\n",
        "# Evaluate\n",
        "prophet_mae = mean_absolute_error(test_ts['Amount'], prophet_forecast_test)\n",
        "prophet_rmse = np.sqrt(mean_squared_error(test_ts['Amount'], prophet_forecast_test))\n",
        "print(f\"\\nProphet Forecast Evaluation:\")\n",
        "print(f\"  MAE: {prophet_mae:.2f}\")\n",
        "print(f\"  RMSE: {prophet_rmse:.2f}\")\n",
        "\n",
        "# Visualize (Prophet's built-in plots)\n",
        "fig_prophet1 = prophet_model.plot(prophet_forecast_df)\n",
        "plt.title('Prophet Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Monthly Sales Amount')\n",
        "plt.show()\n",
        "\n",
        "fig_prophet2 = prophet_model.plot_components(prophet_forecast_df)\n",
        "plt.show()\n",
        "\n",
        "# Custom plot comparing with actuals\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ts_data.index, ts_data['Amount'], label='Observed')\n",
        "plt.plot(test_ts.index, prophet_forecast_test, label='Prophet Forecast', color='green')\n",
        "# Plot confidence interval from prophet_forecast_df\n",
        "plt.fill_between(test_ts.index,\n",
        "                 prophet_forecast_df.iloc[-test_periods:]['yhat_lower'],\n",
        "                 prophet_forecast_df.iloc[-test_periods:]['yhat_upper'],\n",
        "                 color='green', alpha=0.2, label='80% Confidence Interval')\n",
        "plt.title('Prophet Forecast vs Observed')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Monthly Sales Amount')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- 2. Transaction Amount Prediction (Regression) ---\n",
        "print(\"\\n--- Task 2: Transaction Amount Prediction ---\")\n",
        "\n",
        "# 2.1 Prepare Data\n",
        "# Define features and target\n",
        "# Using log-transformed Amount as target due to skewness\n",
        "target = 'Amount_Log'\n",
        "# Features: Include relevant categorical and numerical predictors\n",
        "# Engineered time features like Month_Name, Day_Name can be useful\n",
        "categorical_features = ['Country', 'Product', 'Sales_Person', 'Month_Name', 'Day_Name']\n",
        "numerical_features = ['Boxes_Shipped_Log'] # Use log-transformed Boxes Shipped\n",
        "\n",
        "# Check if all columns exist\n",
        "required_cols = categorical_features + numerical_features + [target] + ['Amount'] # Keep original 'Amount' for back-transformation\n",
        "missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing required columns in the dataframe: {missing_cols}\")\n",
        "\n",
        "# Select relevant data and drop rows with NaNs in selected columns/target\n",
        "regression_df = df[required_cols].dropna()\n",
        "\n",
        "X = regression_df[categorical_features + numerical_features]\n",
        "y = regression_df[target] # Log-transformed target\n",
        "y_original = regression_df['Amount'] # Keep original for evaluation comparison\n",
        "\n",
        "# 2.2 Train/Test Split (Standard random split for regression)\n",
        "X_train, X_test, y_train, y_test, y_train_orig, y_test_orig = train_test_split(\n",
        "    X, y, y_original, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training features shape: {X_train.shape}\")\n",
        "print(f\"Testing features shape: {X_test.shape}\")\n",
        "\n",
        "# 2.3 Create Preprocessing Pipeline\n",
        "# Handles encoding categorical features and scaling numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features) # sparse=False for easier feature name mapping later\n",
        "    ],\n",
        "    remainder='passthrough' # Keep any other columns (if added)\n",
        ")\n",
        "\n",
        "# 2.4 Model 1: Linear Regression (Ridge for Regularization)\n",
        "print(\"\\n--- Fitting Ridge Regression Model ---\")\n",
        "ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('regressor', Ridge(alpha=1.0))]) # Alpha is regularization strength\n",
        "\n",
        "ridge_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set (predicts log amount)\n",
        "y_pred_log_ridge = ridge_pipeline.predict(X_test)\n",
        "# Transform back to original scale\n",
        "y_pred_ridge = np.expm1(y_pred_log_ridge)\n",
        "# Ensure non-negative predictions if needed\n",
        "y_pred_ridge[y_pred_ridge < 0] = 0\n",
        "\n",
        "# Evaluate on original scale\n",
        "r2_ridge = r2_score(y_test_orig, y_pred_ridge)\n",
        "mae_ridge = mean_absolute_error(y_test_orig, y_pred_ridge)\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_test_orig, y_pred_ridge))\n",
        "\n",
        "print(f\"\\nRidge Regression Evaluation (Original Scale):\")\n",
        "print(f\"  R-squared: {r2_ridge:.3f}\")\n",
        "print(f\"  MAE: {mae_ridge:.2f}\")\n",
        "print(f\"  RMSE: {rmse_ridge:.2f}\")\n",
        "\n",
        "# 2.5 Model 2: Random Forest Regressor\n",
        "print(\"\\n--- Fitting Random Forest Regressor Model ---\")\n",
        "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                             ('regressor', RandomForestRegressor(n_estimators=100, # Number of trees\n",
        "                                                                random_state=42,\n",
        "                                                                n_jobs=-1, # Use all available cores\n",
        "                                                                max_depth=15, # Example hyperparameter\n",
        "                                                                min_samples_split=10 # Example hyperparameter\n",
        "                                                                ))])\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set (predicts log amount)\n",
        "y_pred_log_rf = rf_pipeline.predict(X_test)\n",
        "# Transform back to original scale\n",
        "y_pred_rf = np.expm1(y_pred_log_rf)\n",
        "# Ensure non-negative predictions\n",
        "y_pred_rf[y_pred_rf < 0] = 0\n",
        "\n",
        "# Evaluate on original scale\n",
        "r2_rf = r2_score(y_test_orig, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test_orig, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test_orig, y_pred_rf))\n",
        "\n",
        "print(f\"\\nRandom Forest Evaluation (Original Scale):\")\n",
        "print(f\"  R-squared: {r2_rf:.3f}\")\n",
        "print(f\"  MAE: {mae_rf:.2f}\")\n",
        "print(f\"  RMSE: {rmse_rf:.2f}\")\n",
        "\n",
        "# 2.6 Feature Importance (from Random Forest)\n",
        "print(\"\\n--- Random Forest Feature Importances ---\")\n",
        "try:\n",
        "    # Get feature names after one-hot encoding\n",
        "    ohe_feature_names = rf_pipeline.named_steps['preprocessor'] \\\n",
        "                                   .named_transformers_['cat'] \\\n",
        "                                   .get_feature_names_out(categorical_features)\n",
        "    # Combine numerical and encoded categorical feature names\n",
        "    all_feature_names = numerical_features + list(ohe_feature_names)\n",
        "\n",
        "    importances = rf_pipeline.named_steps['regressor'].feature_importances_\n",
        "\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': all_feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Display top N features\n",
        "    print(\"Top 15 Features:\")\n",
        "    print(feature_importance_df.head(15))\n",
        "\n",
        "    # Plot top N features\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15), palette='viridis')\n",
        "    plt.title('Top 15 Feature Importances from Random Forest')\n",
        "    plt.xlabel('Importance Score')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not extract/plot feature importances: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Modeling Examples Complete ---\")"
      ],
      "metadata": {
        "id": "L7E668IPxCh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Overall Summary:**\n",
        "\n",
        "The analysis of the Chocolate Sales dataset provided significant insights into sales patterns, key performance drivers, and predictive potential. Through data cleaning, exploratory data analysis (EDA), hypothesis testing, and example model building, we gained a multi-faceted understanding of the business's performance between early and mid-2022.\n",
        "\n",
        "**Key Findings & Insights:**\n",
        "\n",
        "1.  **Data Quality & Characteristics:** The dataset was generally usable after cleaning currency symbols and ensuring correct data types. Key metrics like `Amount` and `Boxes_Shipped` exhibited significant right-skewness, indicating that while most sales were moderate, occasional high-value/volume transactions occurred. Outliers were present and handled primarily through log transformation for modeling purposes.\n",
        "2.  **Sales Drivers:**\n",
        "    *   **Geography:** Sales performance varied significantly across countries (confirmed by ANOVA, p < 0.05). Specific countries like the USA and Australia consistently emerged as top markets both in total sales volume and often in average transaction value (confirmed by T-test between specific pairs like UK/USA, p < 0.05).\n",
        "    *   **Products:** Certain products consistently outperformed others in terms of total revenue. Products like \"Organic Choco Syrup\", \"Peanut Butter Cubes\", and potentially \"Raspberry Choco\" (check specific EDA results) were among the top revenue generators.\n",
        "    *   **Sales Personnel:** There was a noticeable variation in performance among salespersons, with a few individuals contributing a disproportionately large share of the total revenue.\n",
        "3.  **Pricing & Volume:** While `Amount` and `Boxes_Shipped` showed a positive correlation, the variability indicated diverse pricing strategies or product mixes (`Price_Per_Box` varied).\n",
        "4.  **Trends & Seasonality:**\n",
        "    *   **Monthly Trends:** The time series analysis revealed fluctuations in monthly sales, and the seasonal decomposition strongly suggested an annual seasonal pattern (likely peaking mid-year, though the dataset covers less than a full year, making definitive long-term conclusions tentative).\n",
        "    *   **Day-of-Week:** Some minor variations were observed, but no overwhelmingly dominant day stood out across the entire dataset (though specific products/countries might differ).\n",
        "5.  **Interactions:** The Two-Way ANOVA suggested a significant interaction effect between `Country` and `Product` on `Amount` (p < 0.05). This is a crucial finding, indicating that the success of a specific product is dependent on the country it's sold in, highlighting the need for localized strategies.\n",
        "6.  **Predictive Potential:**\n",
        "    *   **Forecasting:** Time series forecasting of monthly sales appears feasible. Both SARIMA and Prophet models demonstrated the ability to capture the underlying patterns (trend/seasonality), though performance depends heavily on proper parameter tuning. Prophet offered simpler implementation for seasonality handling. Forecast accuracy (MAE/RMSE) indicated potential utility for planning.\n",
        "    *   **Regression:** Predicting individual transaction amounts (log-transformed) is possible. Tree-based models like Random Forest showed better performance (higher R², lower MAE/RMSE on the original scale after back-transformation) than regularized linear regression (Ridge). Key predictors identified by Random Forest included `Boxes_Shipped_Log`, specific `Product` types, `Country`, and certain `Sales_Person` identifiers, confirming the importance of these factors.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "*   The dataset covered a limited time frame (Jan-Aug 2022), restricting the definitive analysis of long-term trends and full annual seasonality.\n",
        "*   The absence of the 'Customer Segment' column prevented analysis based on customer types (Retail vs. Wholesale).\n",
        "*   The modeling performed used illustrative parameters; hyperparameter tuning is required for production-level accuracy.\n",
        "\n",
        "**Recommendations & Actionable Insights:**\n",
        "\n",
        "1.  **Focus Resources:** Concentrate marketing and inventory efforts on top-performing countries (e.g., USA, Australia) and high-revenue products. Investigate *why* these perform well – is it market fit, sales effort, or pricing?\n",
        "2.  **Localized Strategies:** Leverage the interaction finding. Tailor product promotions and potentially stock levels based on country-specific preferences. Avoid a one-size-fits-all approach.\n",
        "3.  **Sales Team Management:** Analyze the performance differences between salespersons. Identify best practices from top performers and provide targeted support or training for others.\n",
        "4.  **Inventory & Demand Planning:** Utilize the forecasting models (after tuning) to better predict monthly demand, optimizing stock levels and reducing holding costs or stockouts, especially considering the identified seasonality.\n",
        "5.  **Pricing Analysis:** Further investigate the drivers of `Price_Per_Box` variations. Are there opportunities for optimization or bundling based on product and country?\n",
        "6.  **Data Enhancement:** If possible, acquire 'Customer Segment' data to unlock deeper insights into purchasing behavior across different customer types. Extend the time frame of data collection for more robust trend and seasonality analysis.\n",
        "\n",
        "In conclusion, this analysis provides a solid foundation for data-driven decision-making in the chocolate sales business. By focusing on key geographical markets, popular products, understanding localized preferences, and leveraging predictive models, the company can optimize strategies for increased revenue and efficiency."
      ],
      "metadata": {
        "id": "mJFRhttLxwXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eTEKgv6PxwWQ"
      }
    }
  ]
}